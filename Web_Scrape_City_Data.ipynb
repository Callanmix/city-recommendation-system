{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, re, time, urllib3, nltk\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source \n",
    "\n",
    "https://www.citytowninfo.com/places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State by State Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = 'https://www.citytowninfo.com/places'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {'state':[], 'state_link':[], 'population':[], 'capital':[], 'capital_link':[]}\n",
    "for row in soup.find('table').find_all('tr')[1:]:\n",
    "    state_dict['state'].append(row.select_one(\"td:nth-of-type(1)\").text)\n",
    "    state_dict['state_link'].append(row.select_one(\"td:nth-of-type(1)\").find('a')['href'])\n",
    "    state_dict['population'].append(row.select_one(\"td:nth-of-type(2)\").text)\n",
    "    state_dict['capital'].append(row.select_one(\"td:nth-of-type(3)\").text)\n",
    "    state_dict['capital_link'].append(row.select_one(\"td:nth-of-type(3)\").find('a')['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyoming - https://www.citytowninfo.com/places/wyoming                                                                   \r"
     ]
    }
   ],
   "source": [
    "state_info_dict = {}\n",
    "for link, state in zip(state_dict['state_link'], state_dict['state']):\n",
    "    print('{} - {} {}\\r'.format(state, link, \" \"*40), end=\"\")\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find('div', attrs = {'id':'population_data'}).find_all('table')\n",
    "    state_info_dict[state] = []\n",
    "    for table in tables:\n",
    "        for row in table.find_all('tr'):\n",
    "            state_info_dict[state].append({row.select_one(\"td:nth-of-type(1)\").text : row.select_one(\"td:nth-of-type(2)\").text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in state_info_dict.keys():\n",
    "    state_info_dict[state] = dict(map(dict.popitem, state_info_dict[state])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data = pd.DataFrame(state_info_dict).transpose()\n",
    "state_data = state_data.dropna(thresh=len(state_data)-1, axis=1).drop(['+ Show More'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City By City Data\n",
    "\n",
    "Here we are grabbing all the links for each of the cities for each of the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 - Wyoming                       \r"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path =\"outputs/chromedriver.exe\") \n",
    "wait = WebDriverWait(driver, 100)\n",
    "\n",
    "city_dict = {}\n",
    "i = 1\n",
    "\n",
    "for link, state in zip(state_dict['state_link'], state_dict['state']):\n",
    "    \n",
    "    print('{} - {}           \\r'.format(i, state), end=\"\")\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "    driver.get(link) \n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    city_dict[state] = {\n",
    "        'city':[],\n",
    "        'city_link':[]\n",
    "    }\n",
    "    \n",
    "    cities_click = driver.find_element_by_class_name('quidget_article_content').find_element_by_xpath('//*[@id=\"top-box\"]').find_elements_by_class_name('city_alphabet_range')\n",
    "    for a in cities_click:\n",
    "        driver.execute_script(\"arguments[0].click();\", a)\n",
    "        table = driver.find_element_by_xpath('//*[@id=\"city_list\"]')\n",
    "        bodies = table.find_elements_by_tag_name(\"tbody\") # get all of the rows in the table\n",
    "        for body in bodies:\n",
    "            rows = body.find_elements_by_tag_name(\"tr\")\n",
    "            for row in rows:\n",
    "                # Get the columns (all the column 2)       \n",
    "                try:\n",
    "                    city_dict[state]['city'].append(row.find_elements_by_tag_name(\"td\")[0].text)\n",
    "                    city_dict[state]['city_link'].append(row.find_element_by_tag_name('a').get_attribute('href'))\n",
    "                except:\n",
    "                    pass\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_data = json.dumps(city_dict)\n",
    "f = open(\"outputs/dict_of_city_links.json\",\"w\")\n",
    "f.write(json_data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/dict_of_city_links.json') as f: \n",
    "    city_dict = json.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get City By City Data\n",
    "\n",
    "Now we can use the links that we grabbed and scrape the data from each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51) Wyoming: City 95 of 95 -- Yoder (https://www.citytowninfo.com/places/wyoming/yoder)                                                                                                                                                                                         \r"
     ]
    }
   ],
   "source": [
    "city_info_by_state = {}\n",
    "for state_num, state in enumerate(city_dict.keys()):\n",
    "    city_info_by_state[state] = {}\n",
    "    i = 1\n",
    "    for link, city in zip(city_dict[state]['city_link'], city_dict[state]['city']):\n",
    "        print('{}) {}: City {} of {} -- {} ({}){}\\r'.format(state_num+1, state, i, len(city_dict[state]['city']), city, link, \" \"*80), end=\"\")\n",
    "        i += 1\n",
    "        \n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        city_info_by_state[state][city] = []\n",
    "        \n",
    "        try:\n",
    "            tables = soup.find('div', attrs = {'id':'population_data'}).find_all('table')\n",
    "            for table in tables:\n",
    "                for row in table.find_all('tr'):\n",
    "                    city_info_by_state[state][city].append({row.select_one(\"td:nth-of-type(1)\").text : row.select_one(\"td:nth-of-type(2)\").text})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            tables = soup.find('div', attrs = {'id':'almanac'}).find('table')\n",
    "            for table in tables:\n",
    "                for row in table.find_all('tr'):\n",
    "                    city_info_by_state[state][city].append({row.select_one(\"td:nth-of-type(1)\").text : row.select_one(\"td:nth-of-type(2)\").text})\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            div = soup.find('div', attrs = {'id':'climate'})\n",
    "            table = div.findNext('table') # Find the first <table> tag that follows it\n",
    "            rows = table.findAll('tr')\n",
    "            for row in rows:\n",
    "                city_info_by_state[state][city].append({row.select_one(\"td:nth-of-type(1)\").text : row.select_one(\"td:nth-of-type(2)\").text})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "for state in city_info_by_state.keys():\n",
    "    for city in city_info_by_state[state].keys():\n",
    "        city_info_by_state[state][city] = dict(map(dict.popitem, city_info_by_state[state][city])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_data = json.dumps(city_info_by_state)\n",
    "f = open(\"outputs/dict_of_city_data.json\",\"w\")\n",
    "f.write(json_data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/dict_of_city_data.json') as f: \n",
    "    city_info_by_state = json.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Dict to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        (i,j): city_info_by_state[i][j] \n",
    "        for i in city_info_by_state.keys() \n",
    "        for j in city_info_by_state[i].keys()\n",
    "    }, \n",
    "    orient='index'\n",
    ")\n",
    "percent_to_keep = .8\n",
    "city_data = city_data.dropna(thresh=int(percent_to_keep*len(city_data)), axis = 1).drop(['+ Show More', 'Station', 'Distance', ' Category'], axis = 1).dropna(subset=['Total Population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data.to_csv('outputs/all_city_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Transform Raw CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = pd.read_csv('outputs/all_city_data.csv')\n",
    "city_data = city_data.rename(columns={'Unnamed: 0': 'State', 'Unnamed: 1': 'City'})\n",
    "city_data = city_data.replace({'%': '', '\\$': '', ',':''}, regex=True).replace({r'^\\s*$':np.nan, 'nan':np.nan}, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a lot of extra symbols in the data like % and \\$ so I took those out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [column for column in city_data.drop(['Time Zone', 'State', 'City'], axis=1).columns if city_data[column].dtype == 'object']:\n",
    "    test_list = []\n",
    "    for row in city_data[col]:\n",
    "        try:\n",
    "            number = re.findall(r\"(?<![a-zA-Z:])[-+]?\\d*\\.?\\d+\", row)\n",
    "            if len(number) == 1:\n",
    "                test_list.append(number[0])\n",
    "            else:\n",
    "                test_list.append(number)\n",
    "        except:\n",
    "            test_list.append(np.nan)\n",
    "    city_data[col] = test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I parsed the numbers from each of the cells, expect those that were obviously objects. Some had more than 1 number so those went into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#city_data[['Oct Temp','Jul Temp','Apr Temp','Jan Temp']] \n",
    "for column, season in zip(['Oct Temp','Jul Temp','Apr Temp','Jan Temp'], ['Fall','Summer','Spring','Winter']):\n",
    "    low = []\n",
    "    high = []\n",
    "    for row in city_data[column]:\n",
    "        if str(type(row)) == \"<class 'float'>\":\n",
    "            low.append(np.nan)\n",
    "            high.append(np.nan)\n",
    "        else:\n",
    "            low.append(row[0])\n",
    "            high.append(row[1])\n",
    "    names = [season + ' Low', season + ' High']\n",
    "    city_data[names[0]] = low\n",
    "    city_data[names[1]] = high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now brake apart those lists. It turns out it was only the temps (high and low). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calla\\Anaconda3\\envs\\pyMain\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "city_data = city_data.convert_objects(convert_numeric=True).drop(['Oct Temp','Jul Temp','Apr Temp','Jan Temp'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a lot of work, but now I can convert the numbers from object to floats and ints. Columns that had NaNs were assigned to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_columns = ['Male Share of the Population','Female Share of the Population', 'Senior Citizens',\n",
    "       '% of people married','Population % with Bachelor Degree or Higher', \n",
    "       '% Above Poverty Level', '% Below Poverty Level', '% Working from Home',\n",
    "       '% Walking and Biking to Work', '% Using Public Transportation','People Living Alone',\n",
    "       'Other(often includes Hispanic and African American)', 'German',\n",
    "       'Irish', 'French Except Basque', 'Scottish', 'Italian', 'Dutch','English']\n",
    "\n",
    "for key, value in city_data[percentage_columns].iteritems():\n",
    "    test_list = []\n",
    "    for x in value:\n",
    "        test_list.append(x/100)\n",
    "    city_data[key] = test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I converted the percentages from number between 1-100 to 0-1 by dividing those numbers by 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resave Clean City Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data.to_csv('static/data/all_city_data_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
